<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width"><meta name="theme-color" content="#555" media="(prefers-color-scheme: light)"><meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)"><meta name="generator" content="Hexo 6.3.0"><link rel="preconnect" href="https://cdn.bootcdn.net" crossorigin><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#555"><meta name="msvalidate.01" content="A785C89C0562F81A78EC2B5B46029C8A"><meta name="baidu-site-verification" content="codeva-3QuUg959rd"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.css" integrity="sha256-gkQVf8UKZgQ0HyuxL/VnacadJ+D2Kox2TCEBuNQg5+w=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/pace/1.2.4/themes/silver/pace-theme-flash.css"><script src="https://cdn.bootcdn.net/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script><script class="next-config" data-name="main" type="application/json">{"hostname":"blog.canyue.top","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.19.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":17,"offset":12},"copycode":{"enable":true,"style":"default"},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#555","save":"auto"},"mediumzoom":false,"lazyload":true,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":true,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"menu_item":"fadeInLeft","post_block":"fadeInUp","post_header":"fadeInTop","post_body":"fadeIn","coll_header":null,"sidebar":"fadeInUp","All available transition variants":"https://theme-next.js.org/animate/"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":true,"preload":false}}</script><script src="/js/config.js"></script><meta name="description" content="RDDRDD（Resilient Distributed Dataset）即弹性分布式数据集，数据集的全部或部分可以存储在集群的多台机子的内存上，当内存不够时，数据也可持久化到硬盘 RDD的主要特征：  RDD都是只读的，但可以将RDD转换为新的RDD RDD是可分区的，每个分区对应一个Tesk执行 对RDD的操作，相对于对RDD某个分区操作 RDD拥有一系列的分区技术函数，称为算子 RDD之间存"><meta property="og:type" content="article"><meta property="og:title" content="SparkRDD"><meta property="og:url" content="https://blog.canyue.top/article/SparkRDD/index.html"><meta property="og:site_name" content="Canyue&#39;s house"><meta property="og:description" content="RDDRDD（Resilient Distributed Dataset）即弹性分布式数据集，数据集的全部或部分可以存储在集群的多台机子的内存上，当内存不够时，数据也可持久化到硬盘 RDD的主要特征：  RDD都是只读的，但可以将RDD转换为新的RDD RDD是可分区的，每个分区对应一个Tesk执行 对RDD的操作，相对于对RDD某个分区操作 RDD拥有一系列的分区技术函数，称为算子 RDD之间存"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-06-18T01:34:47.000Z"><meta property="article:modified_time" content="2023-06-18T06:28:40.020Z"><meta property="article:author" content="Canyue"><meta property="article:tag" content="Spark"><meta property="article:tag" content="大数据"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://blog.canyue.top/article/SparkRDD/"><script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://blog.canyue.top/article/SparkRDD/","path":"article/SparkRDD/","title":"SparkRDD"}</script><script class="next-config" data-name="calendar" type="application/json">""</script><title>SparkRDD | Canyue's house</title><noscript><link rel="stylesheet" href="/css/noscript.css"></noscript><link rel="alternate" href="/atom.xml" title="Canyue's house" type="application/atom+xml"></head><body itemscope itemtype="http://schema.org/WebPage" class="use-motion"><div class="headband"></div><main class="main"><div class="column"><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏" role="button"><span class="toggle-line"></span> <span class="toggle-line"></span> <span class="toggle-line"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><i class="logo-line"></i><p class="site-title">Canyue's house</p><i class="logo-line"></i></a><p class="site-subtitle" itemprop="description">welcome|欢迎来访</p></div><div class="site-nav-right"><div class="toggle popup-trigger" aria-label="搜索" role="button"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul class="main-menu menu"><li class="menu-item menu-item-博客"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>博客</a></li><li class="menu-item menu-item-主页"><a href="/home" rel="section"><i class="fa fa-address-card fa-fw"></i>主页</a></li><li class="menu-item menu-item-关键字"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>关键字</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">1</span></a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" maxlength="80" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close" role="button"><i class="fa fa-times-circle"></i></span></div><div class="search-result-container no-result"><div class="search-result-icon"><i class="fa fa-spinner fa-pulse fa-5x"></i></div></div></div></div></header><aside class="sidebar"><div class="sidebar-inner sidebar-nav-active sidebar-toc-active"><ul class="sidebar-nav"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="sidebar-panel-container"><div class="post-toc-wrap sidebar-panel"><div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#RDD"><span class="nav-number">1.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%9B%E5%BB%BARDD"><span class="nav-number">1.1.</span> <span class="nav-text">创建RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E5%AF%B9%E8%B1%A1%E5%88%9B%E5%BB%BA"><span class="nav-number">1.1.1.</span> <span class="nav-text">通过对象创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E8%BF%87%E6%9C%AC%E5%9C%B0%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA"><span class="nav-number">1.1.2.</span> <span class="nav-text">通过本地文件创建</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%80%9A%E8%BF%87HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%B8%AD%E6%96%87%E4%BB%B6%E5%88%9B%E5%BB%BA"><span class="nav-number">1.1.3.</span> <span class="nav-text">通过HDFS文件系统中文件创建</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E8%BD%AC%E6%8D%A2%E7%AE%97%E5%AD%90"><span class="nav-number">1.2.</span> <span class="nav-text">RDD转换算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#map"><span class="nav-number">1.2.1.</span> <span class="nav-text">map</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#flatMap"><span class="nav-number">1.2.2.</span> <span class="nav-text">flatMap</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#filter"><span class="nav-number">1.2.3.</span> <span class="nav-text">filter</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#reduceByKey"><span class="nav-number">1.2.4.</span> <span class="nav-text">reduceByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#groupByKey"><span class="nav-number">1.2.5.</span> <span class="nav-text">groupByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#union"><span class="nav-number">1.2.6.</span> <span class="nav-text">union</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sortBy"><span class="nav-number">1.2.7.</span> <span class="nav-text">sortBy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#sortByKey"><span class="nav-number">1.2.8.</span> <span class="nav-text">sortByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#join"><span class="nav-number">1.2.9.</span> <span class="nav-text">join</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#leftOuterJoin"><span class="nav-number">1.2.10.</span> <span class="nav-text">leftOuterJoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#rightOuterJoin"><span class="nav-number">1.2.11.</span> <span class="nav-text">rightOuterJoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#fullOuterJoin"><span class="nav-number">1.2.12.</span> <span class="nav-text">fullOuterJoin</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#intersection"><span class="nav-number">1.2.13.</span> <span class="nav-text">intersection</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#cogroup"><span class="nav-number">1.2.14.</span> <span class="nav-text">cogroup</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#distinct"><span class="nav-number">1.2.15.</span> <span class="nav-text">distinct</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E8%A1%8C%E5%8A%A8%E7%AE%97%E5%AD%90"><span class="nav-number">1.3.</span> <span class="nav-text">RDD行动算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#reduce"><span class="nav-number">1.3.1.</span> <span class="nav-text">reduce</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#count"><span class="nav-number">1.3.2.</span> <span class="nav-text">count</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#countByKey"><span class="nav-number">1.3.3.</span> <span class="nav-text">countByKey</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#take-n"><span class="nav-number">1.3.4.</span> <span class="nav-text">take(n)</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%8C%BA"><span class="nav-number">1.4.</span> <span class="nav-text">分区</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F"><span class="nav-number">1.4.1.</span> <span class="nav-text">分区数量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8B%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">查看分区数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8C%87%E5%AE%9A%E5%88%86%E5%8C%BA%E6%95%B0%E9%87%8F"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">指定分区数量</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E5%8C%BA%E5%99%A8"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">自定义分区器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">1.5.</span> <span class="nav-text">RDD持久化</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E7%BA%A7%E5%88%AB"><span class="nav-number">1.5.1.</span> <span class="nav-text">存储级别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%8C%81%E4%B9%85%E5%8C%96"><span class="nav-number">1.5.2.</span> <span class="nav-text">持久化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%93%E5%AD%98"><span class="nav-number">1.5.3.</span> <span class="nav-text">缓存</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD%E6%A3%80%E6%9F%A5%E7%82%B9"><span class="nav-number">1.6.</span> <span class="nav-text">RDD检查点</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B1%E4%BA%AB%E5%8F%98%E9%87%8F"><span class="nav-number">1.7.</span> <span class="nav-text">共享变量</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%BB%98%E8%AE%A4%E7%9A%84%E5%8F%98%E9%87%8F%E4%BC%A0%E9%80%92"><span class="nav-number">1.7.1.</span> <span class="nav-text">默认的变量传递</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F"><span class="nav-number">1.7.2.</span> <span class="nav-text">广播变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B4%AF%E5%8A%A0%E5%99%A8"><span class="nav-number">1.7.3.</span> <span class="nav-text">累加器</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A9%E5%85%85%E6%A0%B7%E4%BE%8B"><span class="nav-number">1.8.</span> <span class="nav-text">扩充样例</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkRDD%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8F"><span class="nav-number">1.8.1.</span> <span class="nav-text">SparkRDD二次排序</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#SparkRDD%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3"><span class="nav-number">1.8.2.</span> <span class="nav-text">SparkRDD数据倾斜问题解决</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%A7%A3%E5%86%B3%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%E7%9A%84%E5%87%A0%E4%B8%AA%E6%96%B9%E6%B3%95"><span class="nav-number">1.8.2.1.</span> <span class="nav-text">解决数据倾斜的几个方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A1%88%E4%BE%8B"><span class="nav-number">1.8.2.2.</span> <span class="nav-text">案例</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="site-author-image" itemprop="image" alt="Canyue" src="/../image/avatar.jpg"><p class="site-author-name" itemprop="name">Canyue</p><div class="site-description" itemprop="description">梳理知识的同时,希望也能帮助到你</div></div><div class="site-state-wrap animated"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">10</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">1</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">13</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="links-of-author animated"><span class="links-of-author-item"><a href="https://github.com/1024canyue" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;1024canyue" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a> </span><span class="links-of-author-item"><a href="https://gitee.com/canyue2048" title="Gitee → https:&#x2F;&#x2F;gitee.com&#x2F;canyue2048" rel="noopener me" target="_blank"><i class="fa fa-code fa-fw"></i>Gitee</a> </span><span class="links-of-author-item"><a href="https://tuchong.com/20603255" title="图虫 → https:&#x2F;&#x2F;tuchong.com&#x2F;20603255" rel="noopener me" target="_blank"><i class="fa fa-camera fa-fw"></i>图虫</a> </span><span class="links-of-author-item"><a href="mailto:lintaisheng@outlook.com" title="E-Mail → mailto:lintaisheng@outlook.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a></span></div><div id="diyClock"><canvas id="canvas" style="width:60%;margin-top:9px;padding:0"></canvas></div><div style="font-size:1.2rem"><a target="_blank" class="social-link" href="/atom.xml"><span class="icon"><i class="fa fa-rss"></i> </span><span class="label">RSS订阅</span></a></div><div class="cc-license animated" itemprop="license"><a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh_CN" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.bootcdn.net/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a></div><script async>!function(){var l,s=8,f=[[[0,0,1,1,1,0,0],[0,1,1,0,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,0,1,1,0],[0,0,1,1,1,0,0]],[[0,0,0,1,1,0,0],[0,1,1,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[1,1,1,1,1,1,1]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,0,0,1,1],[1,1,1,1,1,1,1]],[[1,1,1,1,1,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,1,1,1,0,0],[0,0,0,0,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,0,1,1,1,0],[0,0,1,1,1,1,0],[0,1,1,0,1,1,0],[1,1,0,0,1,1,0],[1,1,1,1,1,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,1,1]],[[1,1,1,1,1,1,1],[1,1,0,0,0,0,0],[1,1,0,0,0,0,0],[1,1,1,1,1,1,0],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,0,0,0,1,1,0],[0,0,1,1,0,0,0],[0,1,1,0,0,0,0],[1,1,0,0,0,0,0],[1,1,0,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[1,1,1,1,1,1,1],[1,1,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,0,0,1,1,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0],[0,0,1,1,0,0,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,1,1,0]],[[0,1,1,1,1,1,0],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[1,1,0,0,0,1,1],[0,1,1,1,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,0,1,1],[0,0,0,0,1,1,0],[0,0,0,1,1,0,0],[0,1,1,0,0,0,0]],[[0,0,0,0],[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0],[0,1,1,0],[0,1,1,0],[0,0,0,0],[0,0,0,0]]];function n(n){var t=[],e=(l.fillStyle="#aaa",new Date),u=60,a=e.getHours(),h=Math.floor(a/10),a=a%10,o=(t.push({num:h}),t.push({num:a}),t.push({num:10}),e.getMinutes()),h=Math.floor(o/10),a=o%10,o=(t.push({num:h}),t.push({num:a}),t.push({num:10}),e.getSeconds()),h=Math.floor(o/10),a=o%10;t.push({num:h}),t.push({num:a});for(var r=0;r<t.length;r++)u=function(n,t,e,u){for(var a=f[e],h=0;h<a.length;h++)for(var o=0;o<a[h].length;o++)1==a[h][o]&&(u.beginPath(),u.arc(n+s+2*s*o,t+s+2*s*h,s,0,2*Math.PI),u.fill());return u.beginPath(),n+=a[0].length*s*2}(t[r].offsetX=u,30,t[r].num,n),r<t.length-1&&10!=t[r].num&&10!=t[r+1].num&&(u+=25)}var t=document.getElementById("canvas");t.width=950,t.height=200,l=t.getContext("2d"),new Date;setInterval(function(){l.clearRect(0,0,l.canvas.width,l.canvas.height),n(l)},50)}()</script></div></div></div></aside></div><div class="main-inner post posts-expand"><div class="post-block"><article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://blog.canyue.top/article/SparkRDD/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/../image/avatar.jpg"><meta itemprop="name" content="Canyue"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Canyue's house"><meta itemprop="description" content="梳理知识的同时,希望也能帮助到你"></span><span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork"><meta itemprop="name" content="SparkRDD | Canyue's house"><meta itemprop="description" content=""></span><header class="post-header"><h1 class="post-title" itemprop="name headline">SparkRDD</h1><div class="post-meta-container"><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-06-18 09:34:47 / 修改时间：14:28:40" itemprop="dateCreated datePublished" datetime="2023-06-18T09:34:47+08:00">2023-06-18</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-folder"></i> </span><span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/%E7%AC%94%E8%AE%B0%E5%88%86%E4%BA%AB/" itemprop="url" rel="index"><span itemprop="name">笔记分享</span></a> </span></span><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>3.5k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></div></header><div class="post-body" itemprop="articleBody"><h1 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h1><p>RDD（Resilient Distributed Dataset）即弹性分布式数据集，数据集的全部或部分可以存储在集群的多台机子的内存上，当内存不够时，数据也可持久化到硬盘</p><p>RDD的主要特征：</p><ul><li>RDD都是<code>只读</code>的，但可以将RDD转换为新的RDD</li><li>RDD是可分区的，每个分区对应一个Tesk执行</li><li>对RDD的操作，相对于对RDD某个分区操作</li><li>RDD拥有一系列的分区技术函数，称为算子</li><li>RDD之间存在依赖关系，可以实现管道化</li></ul><p><code>转换算子</code>负责对RDD中的数据进行计算并转换为一个新的RDD</p><p>Spark中所有算子都是惰性的，只有遇到<code>行动算子</code>才会一起执行</p><span id="more"></span><blockquote><p>下面先使用Spark Shell操作</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]<span class="comment"># spark-shell</span></span><br><span class="line">Spark context Web UI available at http://192.168.159.136:4041</span><br><span class="line">Spark context available as <span class="string">&#x27;sc&#x27;</span> (master = <span class="built_in">local</span>[*], app <span class="built_in">id</span> = local-1665305112084).</span><br><span class="line">Spark session available as <span class="string">&#x27;spark&#x27;</span>.</span><br><span class="line">Welcome to</span><br><span class="line">____              __</span><br><span class="line">/ __/__  ___ _____/ /__</span><br><span class="line">_\ \/ _ \/ _ `/ __/  <span class="string">&#x27;_/</span></span><br><span class="line"><span class="string">/___/ .__/\_,_/_/ /_/\_\   version 2.1.1</span></span><br><span class="line"><span class="string">/_/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_221)</span></span><br><span class="line"><span class="string">Type in expressions to have them evaluated.</span></span><br><span class="line"><span class="string">Type :help for more information.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">scala&gt;</span></span><br></pre></td></tr></table></figure></blockquote><h2 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h2><h3 id="通过对象创建"><a href="#通过对象创建" class="headerlink" title="通过对象创建"></a>通过对象创建</h3><p>Spark可以通过parallelize()或makeRDD()将一个对象集合转换为RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="comment">// 创建Scala集合</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> list = <span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line">list: <span class="type">List</span>[<span class="type">Int</span>] = <span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(list)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res0: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="通过本地文件创建"><a href="#通过本地文件创建" class="headerlink" title="通过本地文件创建"></a>通过本地文件创建</h3><p>Spark可以通过textFile()读取本地文件系统文件，并按行拆分，转换为RDD</p><p>创建一个文本文件，在&#x2F;root&#x2F;text.txt</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# cat text.txt</span><br><span class="line">hello scala</span><br><span class="line">hello spark</span><br><span class="line">hello bigData</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;/root/text.txt&quot;</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /root/text.txt <span class="type">MapPartitionsRDD</span>[<span class="number">4</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res2: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello scala, hello spark, hello bigData)</span><br></pre></td></tr></table></figure><h3 id="通过HDFS文件系统中文件创建"><a href="#通过HDFS文件系统中文件创建" class="headerlink" title="通过HDFS文件系统中文件创建"></a>通过HDFS文件系统中文件创建</h3><p>与前者类似，就在路径前加上<code>hdfs://hostname:post</code>就行</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;hdfs://master:9000/text.txt&quot;</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = hdfs:<span class="comment">//master:9000/text.txt MapPartitionsRDD[12] at textFile at &lt;console&gt;:24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.collect</span><br><span class="line">res6: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(hello scala, hello spark, hello bigData)</span><br></pre></td></tr></table></figure><h2 id="RDD转换算子"><a href="#RDD转换算子" class="headerlink" title="RDD转换算子"></a>RDD转换算子</h2><h3 id="map"><a href="#map" class="headerlink" title="map"></a>map</h3><p>Map算子可以对RDD中每个元素进行转换，如何作为结果RDD中对应位置元素的值</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">15</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRdd = rdd.map(_+<span class="number">1</span>)</span><br><span class="line">newRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">16</span>] at map at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRdd.collect</span><br><span class="line">res9: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure><blockquote><p>RDD是只读的，所以算子对RDD的修改不会影响源RDD，但会返回一个新的RDD</p></blockquote><h3 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h3><p>与map类似，不过flatMap算子对每个元素的转换的结果可以是0、1或多个结果，然后将所有结果合并到一个RDD中</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;a&quot;</span>,<span class="string">&quot;b c&quot;</span>,<span class="string">&quot;d&quot;</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">17</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRdd = rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">newRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">18</span>] at flatMap at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRdd.collect</span><br><span class="line">res10: <span class="type">Array</span>[<span class="type">String</span>] = <span class="type">Array</span>(a, b, c, d)</span><br></pre></td></tr></table></figure><h3 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h3><p>filter算子可以过滤掉一些元素，然后将剩下的元素放入一个新的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">20</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRdd = rdd.filter(_&gt;<span class="number">2</span>)</span><br><span class="line">newRdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">MapPartitionsRDD</span>[<span class="number">21</span>] at filter at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRdd.collect</span><br><span class="line">res11: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">3</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure><h3 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h3><p>reduceByKey只能用于(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code>,默认且只能指定第一个元素为key，第二个为value</p><p>将相同的key的元素进行聚合，value进行计算，然后放在一个新的RDD中返回，返回的类型也是(key,value)</p><p>例,相同key，value求和</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;A&quot;</span>,<span class="number">12</span>),(<span class="string">&quot;A&quot;</span>,<span class="number">13</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">27</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRDD = rdd.reduceByKey((v1,v2) =&gt; v1 + v2)</span><br><span class="line">newRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">28</span>] at reduceByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRDD.collect</span><br><span class="line">res14: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((<span class="type">A</span>,<span class="number">25</span>), (<span class="type">B</span>,<span class="number">3</span>))</span><br></pre></td></tr></table></figure><h3 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h3><p>类似前者，源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>将相同的key的元素聚合，value都丢在一个集合内，然后放在一个新的RDD中返回，返回的类型是(key,CompactBuffer(value…….))</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;A&quot;</span>,<span class="number">12</span>),(<span class="string">&quot;A&quot;</span>,<span class="number">13</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">29</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRDD = rdd.groupByKey()</span><br><span class="line">newRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">ShuffledRDD</span>[<span class="number">30</span>] at groupByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRDD.collect</span><br><span class="line">res15: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = <span class="type">Array</span>((<span class="type">A</span>,<span class="type">CompactBuffer</span>(<span class="number">12</span>, <span class="number">13</span>)), (<span class="type">B</span>,<span class="type">CompactBuffer</span>(<span class="number">1</span>, <span class="number">2</span>)))</span><br></pre></td></tr></table></figure><h3 id="union"><a href="#union" class="headerlink" title="union"></a>union</h3><p>用于将两个<code>相同数据类型</code>RDD进行合并，并放回一个合并后的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">31</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>(<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">32</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.union(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">UnionRDD</span>[<span class="number">33</span>] at union at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res16: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure><h3 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h3><p>sortBy算子可以将RDD内的元素进行排序，并返回新的RDD</p><p>sortBy第一个参数是排序函数，可以用于指定排序依据，第二个参数是布尔值，表示是否升序排序（即如果要降序，就false）</p><p><code>第二个参数默认为true（默认升序）</code></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;xiaomin&quot;</span>,<span class="number">75</span>),(<span class="string">&quot;xiaohua&quot;</span>,<span class="number">85</span>),(<span class="string">&quot;xiaodong&quot;</span>,<span class="number">83</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">57</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRDD = rdd.sortBy(_._2)</span><br><span class="line">newRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">62</span>] at sortBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRDD.collect</span><br><span class="line">res21: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((xiaomin,<span class="number">75</span>), (xiaodong,<span class="number">83</span>), (xiaohua,<span class="number">85</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; newRDD = rdd.sortBy(_._2,<span class="literal">false</span>)</span><br><span class="line">newRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">MapPartitionsRDD</span>[<span class="number">67</span>] at sortBy at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRDD.collect</span><br><span class="line">res22: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">Array</span>((xiaohua,<span class="number">85</span>), (xiaodong,<span class="number">83</span>), (xiaomin,<span class="number">75</span>))</span><br></pre></td></tr></table></figure><h3 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h3><p>源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>按照key进行排序</p><p>只有一个参数表示是否升序排序，默认true,即默认升序</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="number">2</span>,<span class="string">&quot;A&quot;</span>),(<span class="number">1</span>,<span class="string">&quot;B&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;C&quot;</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">75</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRDD = rdd.sortByKey(<span class="literal">false</span>)</span><br><span class="line">newRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">78</span>] at sortByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRDD.collect</span><br><span class="line">res25: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">3</span>,<span class="type">C</span>), (<span class="number">2</span>,<span class="type">A</span>), (<span class="number">1</span>,<span class="type">B</span>))</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> newRDD = rdd.sortByKey()</span><br><span class="line">newRDD: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">ShuffledRDD</span>[<span class="number">81</span>] at sortByKey at &lt;console&gt;:<span class="number">26</span></span><br><span class="line"></span><br><span class="line">scala&gt; newRDD.collect</span><br><span class="line">res26: <span class="type">Array</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = <span class="type">Array</span>((<span class="number">1</span>,<span class="type">B</span>), (<span class="number">2</span>,<span class="type">A</span>), (<span class="number">3</span>,<span class="type">C</span>))</span><br></pre></td></tr></table></figure><h3 id="join"><a href="#join" class="headerlink" title="join"></a>join</h3><p>源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>join算子做的是内连接，是将两个RDD工具key进行连接，并只返回两个RDD都匹配的内容</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;A&quot;</span>,<span class="string">&quot;a&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b&quot;</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">83</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b1&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b2&quot;</span>),(<span class="string">&quot;C&quot;</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">84</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.join(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">MapPartitionsRDD</span>[<span class="number">87</span>] at join at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res27: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">String</span>))] = <span class="type">Array</span>((<span class="type">B</span>,(b,b1)), (<span class="type">B</span>,(b,b2)))</span><br></pre></td></tr></table></figure><h3 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h3><p>源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>leftOuterJoin算子做的是左外连接，是将两个RDD工具key进行连接，左边RDD会都存在，右边RDD只保留匹配内容</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b1&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b2&quot;</span>),(<span class="string">&quot;C&quot;</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">89</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.leftOuterJoin(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">MapPartitionsRDD</span>[<span class="number">92</span>] at leftOuterJoin at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res28: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">A</span>,(a,<span class="type">None</span>)), (<span class="type">B</span>,(b,<span class="type">Some</span>(b1))), (<span class="type">B</span>,(b,<span class="type">Some</span>(b2))))</span><br></pre></td></tr></table></figure><h3 id="rightOuterJoin"><a href="#rightOuterJoin" class="headerlink" title="rightOuterJoin"></a>rightOuterJoin</h3><p>源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>leftOuterJoin算子做的是右外连接，是将两个RDD工具key进行连接，左边RDD只保留匹配内容，右边RDD会都存在</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;A&quot;</span>,<span class="string">&quot;a&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b&quot;</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">93</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b1&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b2&quot;</span>),(<span class="string">&quot;C&quot;</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">94</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.rightOuterJoin(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">String</span>], <span class="type">String</span>))] = <span class="type">MapPartitionsRDD</span>[<span class="number">97</span>] at rightOuterJoin at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res29: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">String</span>], <span class="type">String</span>))] = <span class="type">Array</span>((<span class="type">B</span>,(<span class="type">Some</span>(b),b1)), (<span class="type">B</span>,(<span class="type">Some</span>(b),b2)), (<span class="type">C</span>,(<span class="type">None</span>,c)))</span><br></pre></td></tr></table></figure><h3 id="fullOuterJoin"><a href="#fullOuterJoin" class="headerlink" title="fullOuterJoin"></a>fullOuterJoin</h3><p>源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>leftOuterJoin算子做的是全外连接，是将两个RDD工具key进行连接，两边RDD不管匹不匹配的元素会都存在</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;A&quot;</span>,<span class="string">&quot;a&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b&quot;</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">98</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b1&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b2&quot;</span>),(<span class="string">&quot;C&quot;</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">99</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd3 = rdd1.fullOuterJoin(rdd2)</span><br><span class="line">rdd3: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">String</span>], <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">MapPartitionsRDD</span>[<span class="number">102</span>] at fullOuterJoin at &lt;console&gt;:<span class="number">28</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd3.collect</span><br><span class="line">res30: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Option</span>[<span class="type">String</span>], <span class="type">Option</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">A</span>,(<span class="type">Some</span>(a),<span class="type">None</span>)), (<span class="type">B</span>,(<span class="type">Some</span>(b),<span class="type">Some</span>(b1))), (<span class="type">B</span>,(<span class="type">Some</span>(b),<span class="type">Some</span>(b2))), (<span class="type">C</span>,(<span class="type">None</span>,<span class="type">Some</span>(c))))</span><br></pre></td></tr></table></figure><h3 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h3><p>返回两个RDD元素的<code>交集</code>组成的新RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="number">1</span> to <span class="number">5</span>)</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">105</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="number">3</span> to <span class="number">7</span>)</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">106</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.intersection(rdd2).collect</span><br><span class="line">res32: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">5</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><h3 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h3><p>对两个数据类型是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code>的RDD先按照Key进行组合，然后工具key进行<code>并集</code>操作</p><p>一种合并，左边有的放左边，右边有的放右边，没有留空</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;A&quot;</span>,<span class="string">&quot;a&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b&quot;</span>)))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">117</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd2 = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b1&quot;</span>),(<span class="string">&quot;B&quot;</span>,<span class="string">&quot;b2&quot;</span>),(<span class="string">&quot;C&quot;</span>,<span class="string">&quot;c&quot;</span>)))</span><br><span class="line">rdd2: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">String</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">118</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.cogroup(rdd2).collect</span><br><span class="line">res34: <span class="type">Array</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">String</span>], <span class="type">Iterable</span>[<span class="type">String</span>]))] = <span class="type">Array</span>((<span class="type">A</span>,(<span class="type">CompactBuffer</span>(a),<span class="type">CompactBuffer</span>())), (<span class="type">B</span>,(<span class="type">CompactBuffer</span>(b),<span class="type">CompactBuffer</span>(b1, b2))), (<span class="type">C</span>,(<span class="type">CompactBuffer</span>(),<span class="type">CompactBuffer</span>(c))))</span><br></pre></td></tr></table></figure><h3 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h3><p>返回一个源RDD去重后的RDD</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd1 = sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line">rdd1: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">113</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd1.distinct.collect</span><br><span class="line">res33: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">4</span>, <span class="number">1</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><blockquote><p>⚠<code>注意</code>，会乱序</p></blockquote><h2 id="RDD行动算子"><a href="#RDD行动算子" class="headerlink" title="RDD行动算子"></a>RDD行动算子</h2><p>Spark中对RDD的操作都惰性的，只有遇到行动算子才会触发计算</p><p>所以，在使用转换算子时出现的问题，可能不会立即显现</p><p>行动算子有一些这些</p><ol><li><p>reduce</p></li><li><p>collect</p></li><li><p>count</p></li><li><p>first</p></li><li><p>take</p></li><li><p>takeOrdered</p></li><li><p>aggregate</p></li><li><p>fold</p></li><li><p>countByKey</p></li><li><p>save相关算子</p></li><li><p>foreach</p></li></ol><p>拿几个常用的算子举例</p><h3 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h3><p>将RDD中元素进行聚合,直接返回聚合结果</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">1</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.reduce((a,b) =&gt; a+b)</span><br><span class="line">res0: <span class="type">Int</span> = <span class="number">5050</span></span><br></pre></td></tr></table></figure><blockquote><p>行动算子返回不一定是RDD</p></blockquote><h3 id="count"><a href="#count" class="headerlink" title="count"></a>count</h3><p>统计并返回RDD中元素个数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">2</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.count</span><br><span class="line">res1: <span class="type">Long</span> = <span class="number">100</span></span><br></pre></td></tr></table></figure><h3 id="countByKey"><a href="#countByKey" class="headerlink" title="countByKey"></a>countByKey</h3><p>源RDD数据类型只能是(key,value)样式的scala<code>一维</code>长度为<code>2</code>的<code>元组</code></p><p>按照key分类，然后按照可以统计个数，返回scala.collection.Map</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>((<span class="string">&quot;A&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;B&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;A&quot;</span>,<span class="number">2</span>)))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = <span class="type">ParallelCollectionRDD</span>[<span class="number">3</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.countByKey</span><br><span class="line">res2: scala.collection.<span class="type">Map</span>[<span class="type">String</span>,<span class="type">Long</span>] = <span class="type">Map</span>(<span class="type">A</span> -&gt; <span class="number">2</span>, <span class="type">B</span> -&gt; <span class="number">1</span>)</span><br></pre></td></tr></table></figure><h3 id="take-n"><a href="#take-n" class="headerlink" title="take(n)"></a>take(n)</h3><p>返回RDD前n个元素组成的数组</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">6</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.take(<span class="number">10</span>)</span><br><span class="line">res3: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>)</span><br></pre></td></tr></table></figure><blockquote><p>如果是取第一个，可以使用first算子</p></blockquote><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><p>RDD可以分为多个分区，各个分区放在各个节点上</p><h3 id="分区数量"><a href="#分区数量" class="headerlink" title="分区数量"></a>分区数量</h3><p>RDD中各个分区中的数据可以并行计算，Spark会给每个分区分配一个Task任进行计算</p><p>RDD<code>通常</code>默认分区数量为你的集群的CPU核心数</p><h4 id="查看分区数量"><a href="#查看分区数量" class="headerlink" title="查看分区数量"></a>查看分区数量</h4><p>我可以使用getNumPartitis查看分区数量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">9</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.getNumPartitions</span><br><span class="line">res9: <span class="type">Int</span> = <span class="number">4</span></span><br></pre></td></tr></table></figure><blockquote><p>getNumPartitis默认是使用当前设备的核心数；我是用了台4核心的虚拟机。可以看出，与核心数一致</p></blockquote><h4 id="指定分区数量"><a href="#指定分区数量" class="headerlink" title="指定分区数量"></a>指定分区数量</h4><p>一般来说，在创建RDD的函数的第二个参数传入一个Int类型的数，就可以指定RDD分区数量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>,<span class="number">20</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">10</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.getNumPartitions</span><br><span class="line">res10: <span class="type">Int</span> = <span class="number">20</span></span><br></pre></td></tr></table></figure><blockquote><p>分区数不受核心数限制</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.textFile(<span class="string">&quot;/root/text.txt&quot;</span>,<span class="number">1</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">String</span>] = /root/text.txt <span class="type">MapPartitionsRDD</span>[<span class="number">14</span>] at textFile at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.getNumPartitions</span><br><span class="line">res12: <span class="type">Int</span> = <span class="number">1</span></span><br></pre></td></tr></table></figure><blockquote><p>注意，textFile中第二个参数只是最小分区数量，具体Spark可能会按照文件大小等一些因素决定</p><hr><p>所以，我一开始的话可能有点片面，实际上要看看编辑器的提示或源码，视情况而定</p></blockquote><h4 id="自定义分区器"><a href="#自定义分区器" class="headerlink" title="自定义分区器"></a>自定义分区器</h4><p>分区规则是由分区控制器（Partitioner）控制，Spark的主要分区类是HashPartitioner和RangePartitioner,他们都继承自抽象类Partitioner,我们可以实现Partitioner类达到自定义分区控制器</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">Partitioner</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  @author 20软件林泰圣</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">PartitionerTest</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;PartitionerTest&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> dataRDD:<span class="type">RDD</span>[(<span class="type">String</span>,(<span class="type">String</span>,<span class="type">String</span>))] = sc.makeRDD(</span><br><span class="line">      <span class="type">Array</span>(</span><br><span class="line">        (<span class="string">&quot;cat&quot;</span>,(<span class="string">&quot;mimi&quot;</span>,<span class="string">&quot;white&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;cat&quot;</span>,(<span class="string">&quot;tom&quot;</span>,<span class="string">&quot;blue&quot;</span>)),</span><br><span class="line">        (<span class="string">&quot;dog&quot;</span>,(<span class="string">&quot;wancai&quot;</span>,<span class="string">&quot;black&quot;</span>))</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="comment">//写到HDFS</span></span><br><span class="line">    dataRDD</span><br><span class="line">      .partitionBy(<span class="keyword">new</span> <span class="type">MyPartitioner</span>(<span class="number">3</span>))   <span class="comment">//自定义分区器</span></span><br><span class="line">      .saveAsTextFile(<span class="string">&quot;hdfs://192.168.159.136:9000/output&quot;</span>)    <span class="comment">//保存到hdfs</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区器</span></span><br><span class="line"><span class="comment"> * @param partitionNum 分区数量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span>(<span class="params">partitionNum:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">  <span class="comment">//获得分区数量</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">numPartitions</span></span>: <span class="type">Int</span> = partitionNum</span><br><span class="line">  <span class="comment">//获得分区ID,即那个分区，分区ID为Int类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">getPartition</span></span>(key: <span class="type">Any</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> project = key.toString</span><br><span class="line">    <span class="keyword">if</span>(project.equals(<span class="string">&quot;cat&quot;</span>))&#123;          <span class="comment">//猫：0</span></span><br><span class="line">      <span class="number">0</span></span><br><span class="line">    &#125;<span class="keyword">else</span> <span class="keyword">if</span>(project.equals(<span class="string">&quot;dog&quot;</span>))&#123;    <span class="comment">//狗:1</span></span><br><span class="line">      <span class="number">1</span></span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;                              <span class="comment">//其他:2</span></span><br><span class="line">      <span class="number">2</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop dfs -ls /output</span><br><span class="line"></span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 ÁÖ̩ʥ supergroup          0 2022-10-10 01:18 /output/_SUCCESS</span><br><span class="line">-rw-r--r--   3 ÁÖ̩ʥ supergroup         36 2022-10-10 01:18 /output/part-00000</span><br><span class="line">-rw-r--r--   3 ÁÖ̩ʥ supergroup         21 2022-10-10 01:18 /output/part-00001</span><br><span class="line">-rw-r--r--   3 ÁÖ̩ʥ supergroup          0 2022-10-10 01:18 /output/part-00002</span><br><span class="line">[root@master ~]# hadoop dfs -cat /output/part-00000</span><br><span class="line"></span><br><span class="line">(cat,(mimi,white))</span><br><span class="line">(cat,(tom,blue))</span><br><span class="line">[root@master ~]# hadoop dfs -cat /output/part-00001</span><br><span class="line"></span><br><span class="line">(dog,(wancai,black))</span><br><span class="line">[root@master ~]# hadoop dfs -cat /output/part-00002</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="RDD持久化"><a href="#RDD持久化" class="headerlink" title="RDD持久化"></a>RDD持久化</h2><p>SparkRDD是懒加载的，只有遇到行动算子才会从头开始计算所有RDD，当遇到一个RDD被多次使用，就会严重影响性能，这时候可以通过RDD持久化避免重复计算</p><p>在RDD上进行persist()或cache()可以对RDD进行持久化；cache()底层调用persist()，不可更改级别</p><h3 id="存储级别"><a href="#存储级别" class="headerlink" title="存储级别"></a>存储级别</h3><table><thead><tr><th>Storage Level</th><th>Meaning</th></tr></thead><tbody><tr><td>MEMORY_ONLY</td><td>将RDD作为非序列化的Java对象存储在jvm中。如果RDD不适合存在内存中，一些分区将不会被缓存，从而在每次需要这些分区时都需重新计算它们。这是系统默认的存储级别。</td></tr><tr><td>MEMORY_AND_DISK</td><td>将RDD作为非序列化的Java对象存储在jvm中。如果RDD不适合存在内存中，将这些不适合存在内存中的分区存储在磁盘中，每次需要时读出它们。</td></tr><tr><td>MEMORY_ONLY_SER</td><td>将RDD作为序列化的Java对象存储（每个分区一个byte数组）。这种方式比非序列化方式更节省空间，特别是用到快速的序列化工具时，但是会更耗费cpu资源—密集的读操作。</td></tr><tr><td>MEMORY_AND_DISK_SER</td><td>和MEMORY_ONLY_SER类似，但不是在每次需要时重复计算这些不适合存储到内存中的分区，而是将这些分区存储到磁盘中。</td></tr><tr><td>DISK_ONLY</td><td>仅仅将RDD分区存储到磁盘中</td></tr><tr><td>MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.</td><td>和上面的存储级别类似，但是复制每个分区到集群的两个节点上面</td></tr><tr><td>OFF_HEAP (experimental)</td><td>以序列化的格式存储RDD到<a target="_blank" rel="noopener" href="http://tachyon-project.org/">Tachyon</a>中。相对于MEMORY_ONLY_SER，OFF_HEAP减少了垃圾回收的花费，允许更小的执行者共享内存池。这使其在拥有大量内存的环境下或者多并发应用程序的环境中具有更强的吸引力。</td></tr></tbody></table><h3 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.cache</span><br><span class="line">res13: rdd.<span class="keyword">type</span> = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.persist</span><br><span class="line">res14: rdd.<span class="keyword">type</span> = <span class="type">ParallelCollectionRDD</span>[<span class="number">16</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br></pre></td></tr></table></figure><blockquote><p>默认持久化到内存中，可切换等级调整到硬盘，具体看什上面的存储级别</p></blockquote><blockquote><p>Spark程序执行结束后，cache()与persist()中的内容会被清空</p></blockquote><blockquote><p>cache()与persist()操作也只能在遇到行动算子后才会执行持久化</p></blockquote><blockquote><p>RDD的持久化方法，同样适用于后面会出现的DataFrame以及DataSet</p></blockquote><h3 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h3><p>cache() 是persist的一种 准确说是 persist(StorageLevel.MEMORY_ONLY)。</p><h2 id="RDD检查点"><a href="#RDD检查点" class="headerlink" title="RDD检查点"></a>RDD检查点</h2><p>cache()与persist()中的内容会被清空，无法长期保存</p><p>检查点可以将RDD状态保存在硬盘中，在需要的时候又可以还原</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; sc.setCheckpointDir(<span class="string">&quot;/root/checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; sc.setCheckpointDir(<span class="string">&quot;/root/checkpoint&quot;</span>)    <span class="comment">//设置检查点路径</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="number">1</span> to <span class="number">100</span>)</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">0</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.checkpoint</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# ls -l checkpoint/</span><br><span class="line">总用量 0</span><br><span class="line">drwxr-xr-x 2 root root 6 10月 10 02:58 56326811-6331-4a7c-884b-fb09a2a15954</span><br><span class="line">drwxr-xr-x 2 root root 6 10月 10 02:58 d2a3592b-c36b-4a57-8488-626da9b9d425</span><br></pre></td></tr></table></figure><blockquote><p>检查点只有在遇到行动算子后才会保存</p><p>当下次行动算子计算时，将直接调用检查点数据，不需要从头计算</p></blockquote><h2 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h2><p>通常情况下(集群)，Spark应用程序会将个算子中的函数分配到多个Worker节点运行，若一个算子使用了某个外部变量，那该变量就会被复制到该Worker节点上的每个Task任务中。由于各个Task相互独立，当该变量存储的数据非常大时(例如存储了100M的SCV文件的源数据),那就回导致网络传输以及内存开销明显加大，因此可能会导致些许性能问题</p><p>Spark提供了两种共享变量，广播变量以及累加器,学过Flink这些应该不会陌生</p><h3 id="默认的变量传递"><a href="#默认的变量传递" class="headerlink" title="默认的变量传递"></a>默认的变量传递</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line">arr: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.map(x =&gt; (x,arr)).collect</span><br><span class="line">res4: <span class="type">Array</span>[(<span class="type">String</span>, <span class="type">Array</span>[<span class="type">String</span>])] = <span class="type">Array</span>((<span class="type">A</span>,<span class="type">Array</span>(<span class="type">A</span>, <span class="type">B</span>, <span class="type">C</span>, <span class="type">D</span>)), (<span class="type">B</span>,<span class="type">Array</span>(<span class="type">A</span>, <span class="type">B</span>, <span class="type">C</span>, <span class="type">D</span>)), (<span class="type">C</span>,<span class="type">Array</span>(<span class="type">A</span>, <span class="type">B</span>, <span class="type">C</span>, <span class="type">D</span>)), (<span class="type">D</span>,<span class="type">Array</span>(<span class="type">A</span>, <span class="type">B</span>, <span class="type">C</span>, <span class="type">D</span>)))</span><br></pre></td></tr></table></figure><p>这是个简单的案例，有一个存放在Driver的外部变量arr，arr被用在map算子中，arr将被发送给每个Task</p><p>但要是arr是一个100M的数据，那每个Task去维护100M大小的数据副本，要是某个Executor启动了4个Task，那就共有400M是数据副本，消耗内存不说，还有带来较大的网络开销</p><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量是一个在每个Worder节点的一个只读缓存，该变量无论你有几个Task都只发送一次，每个Worder节点也只存在一个广播变量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> broadcastArr = sc.broadcast(arr)</span><br><span class="line">broadcastArr: org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]] = <span class="type">Broadcast</span>(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; rdd.map(x =&gt; (x,broadcastArr)).collect</span><br><span class="line">res5: <span class="type">Array</span>[(<span class="type">String</span>, org.apache.spark.broadcast.<span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Int</span>]])] = <span class="type">Array</span>((<span class="type">A</span>,<span class="type">Broadcast</span>(<span class="number">2</span>)), (<span class="type">B</span>,<span class="type">Broadcast</span>(<span class="number">2</span>)), (<span class="type">C</span>,<span class="type">Broadcast</span>(<span class="number">2</span>)), (<span class="type">D</span>,<span class="type">Broadcast</span>(<span class="number">2</span>)))</span><br></pre></td></tr></table></figure><p>可以发现，RDD中每个元素使用org.apache.spark.broadcast.Broadcast对象，它只在执行行动算子后复制一次到各个Worder节点,不管你Executor启动了几个Task，这个Worder节点只有一个变量</p><p>Broadcast对象只是一个简单的封装，你可以使用.value方法得到里面的值</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; broadcastArr.value</span><br><span class="line">res7: <span class="type">Array</span>[<span class="type">Int</span>] = <span class="type">Array</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br></pre></td></tr></table></figure><h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>广播变量是将Driver中的变量广播并缓存到各个Worker节点，那累加器就是可以让Worker访问Driver中的特殊变量</p><p>若不是用累加器。由于Driver中的变量是复制到各个Worder的各个Task中，那就有一个问题，在Task中的修改，不影响Driver的外部变量</p><p>如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">var</span> sum = <span class="number">0</span></span><br><span class="line">sum: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">8</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreach(x =&gt; sum += x)</span><br><span class="line"></span><br><span class="line">scala&gt; print(sum)</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure><blockquote><p>Driver中sum值未被更改，更改的仅仅是各个Task中被复制过去的sum</p><p>这时候可以使用累加器，累加器使用add()方法累加，使用value()方法取值</p></blockquote><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> myAcc = sc.longAccumulator(<span class="string">&quot;myAcc&quot;</span>)</span><br><span class="line">myAcc: org.apache.spark.util.<span class="type">LongAccumulator</span> = <span class="type">LongAccumulator</span>(id: <span class="number">480</span>, name: <span class="type">Some</span>(myAcc), value: <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">scala&gt; <span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>))</span><br><span class="line">rdd: org.apache.spark.rdd.<span class="type">RDD</span>[<span class="type">Int</span>] = <span class="type">ParallelCollectionRDD</span>[<span class="number">9</span>] at makeRDD at &lt;console&gt;:<span class="number">24</span></span><br><span class="line"></span><br><span class="line">scala&gt; rdd.foreach(x =&gt; myAcc.add(x))</span><br><span class="line"></span><br><span class="line">scala&gt; print(myAcc.value)</span><br><span class="line"><span class="number">15</span></span><br></pre></td></tr></table></figure><h2 id="扩充样例"><a href="#扩充样例" class="headerlink" title="扩充样例"></a>扩充样例</h2><h3 id="SparkRDD二次排序"><a href="#SparkRDD二次排序" class="headerlink" title="SparkRDD二次排序"></a>SparkRDD二次排序</h3><p>在项目目录下，存在一个文件sort.txt，内容如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">4 5</span><br><span class="line">2 6</span><br><span class="line">4 6</span><br><span class="line">3 5</span><br><span class="line">1 5</span><br><span class="line">8 6</span><br><span class="line">4 7</span><br></pre></td></tr></table></figure><p>要求，每行按照第一个数字升序排序，当第一个数字相同时，按照第二个数字降序排序</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.rdd.<span class="type">RDD</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @author 20软件林泰圣</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Sort01</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="class"><span class="keyword">class</span> <span class="title">SecondSortKey</span>(<span class="params">val firstNum:<span class="type">Int</span>,val secondNum:<span class="type">Int</span></span>) <span class="keyword">extends</span> <span class="title">Ordered</span>[<span class="type">SecondSortKey</span>] <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">compare</span></span>(that: <span class="type">SecondSortKey</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">      <span class="comment">//判断第一个数字是否相同</span></span><br><span class="line">      <span class="keyword">if</span>(<span class="keyword">this</span>.firstNum != that.firstNum)&#123;</span><br><span class="line">        <span class="keyword">this</span>.firstNum - that.firstNum</span><br><span class="line">      &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        that.secondNum - <span class="keyword">this</span>.secondNum</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">&quot;Sort01&quot;</span>)</span><br><span class="line">      .setMaster(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> pairRDD:<span class="type">RDD</span>[(<span class="type">SecondSortKey</span>,<span class="type">String</span>)] = sc</span><br><span class="line">      .textFile(<span class="string">&quot;./sort.txt&quot;</span>)</span><br><span class="line">      .map(d =&gt; &#123;</span><br><span class="line">        (<span class="keyword">new</span> <span class="type">SecondSortKey</span>(</span><br><span class="line">          d.split(<span class="string">&quot; &quot;</span>)(<span class="number">0</span>).toInt,</span><br><span class="line">          d.split(<span class="string">&quot; &quot;</span>)(<span class="number">1</span>).toInt</span><br><span class="line">        ),d)</span><br><span class="line">      &#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sortRDD = pairRDD</span><br><span class="line">      .sortByKey()              <span class="comment">//将按照SecondSortKey排序</span></span><br><span class="line">      .map(_._2)</span><br><span class="line"></span><br><span class="line">    sortRDD.collect().foreach(d =&gt; &#123;</span><br><span class="line">      println(d)</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 5</span><br><span class="line">2 6</span><br><span class="line">3 5</span><br><span class="line">4 7</span><br><span class="line">4 6</span><br><span class="line">4 5</span><br><span class="line">8 6</span><br></pre></td></tr></table></figure><h3 id="SparkRDD数据倾斜问题解决"><a href="#SparkRDD数据倾斜问题解决" class="headerlink" title="SparkRDD数据倾斜问题解决"></a>SparkRDD数据倾斜问题解决</h3><p>案例来自《Spark大数据分析实战》</p><p>如果一个Spark作业有两个Stage(),Stage2依赖与Stage1时，Stage2必须要等另一个Stage1<code>完成时才能往下做</code></p><p>这时要是Stage1的任务量非常庞大，假如要1小时，Stage2只需要1秒，这个任务<code>分配不均</code>的现象就叫数据倾斜</p><p>数据倾斜会影响Spark应用程序的执行效率以及资源利用率</p><h4 id="解决数据倾斜的几个方法"><a href="#解决数据倾斜的几个方法" class="headerlink" title="解决数据倾斜的几个方法"></a>解决数据倾斜的几个方法</h4><ol><li><p>数据预处理</p><p>假设Spark数据都来自Hive或MySQL，那可以先在上面对数据进行预处理，尽量保证数据均匀，或者是先对数据进行一次聚合，在传入Spark时就不要那么多次的reduceByKey()操作,就能减少Shuffle操作，缓解数据倾斜</p></li><li><p>过滤掉导致数据倾斜的key</p><p>要是导致数据倾斜的key本身无意义，本身不参与计算或对结果无影响，那可以讲该key过滤掉</p></li><li><p>提高Shuffle的并行度</p><p>Spark RDD的Shuffer过程与MapReduce类似，会涉及数据重组和从分区，如果并行度设置不合适，那可能会导致多个Key被分配到一个分区，使得某一Task任务过大，影响性能</p><p>在使用聚合算子(xxxByKey相关)时，可以通过参数传入并行度，给原先分多个key的Task的任务分配到多个Task上，缓解问题</p></li><li><p>通过随机Key前缀进行双重聚合</p><p>在相同key中加上随机前缀，使得相同key被拆分到不同的key，就可以让原先分配在一个分区的key分配到多个分区，从而分配到多个key</p></li></ol><h4 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h4><p>有一数据文件，存放在hdfs:&#x2F;&#x2F;master:9000&#x2F;test&#x2F;word.txt</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop dfs -cat /test/word.txt</span><br><span class="line">DEPRECATED: Use of this script to execute hdfs command is deprecated.</span><br><span class="line">Instead use the hdfs command for it.</span><br><span class="line"></span><br><span class="line">hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello spark hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello spark hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello scala hello hello hello hello hello hello hello hadoop hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello spark hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello hello</span><br></pre></td></tr></table></figure><p>数据中hello量过大，若不处理，再进行聚合操作时，将发生数据倾斜</p><p>这时候我们使用随机Key前缀的方法环境数据倾斜问题，通过随机的key，能将量大的Hello分配给多个Stage，这样就避免一个Stage工作量过大的问题</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.<span class="type">SparkSession</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scala.util.<span class="type">Random</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">DataLean</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">      .builder()</span><br><span class="line">      .appName(<span class="string">&quot;DataLean&quot;</span>)</span><br><span class="line">      .master(<span class="string">&quot;local[*]&quot;</span>)</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> wordRDD = sc.textFile(<span class="string">&quot;hdfs://master:9000/test/word.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> result = wordRDD</span><br><span class="line">      .flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">      .map(x =&gt; &#123;                          <span class="comment">//添加前缀</span></span><br><span class="line">        <span class="keyword">val</span> random = <span class="type">Random</span>.nextInt(<span class="number">10</span>)    <span class="comment">//0-9随机数</span></span><br><span class="line">        (random + <span class="string">&quot;_&quot;</span> + x ,<span class="number">1</span>)</span><br><span class="line">      &#125;)</span><br><span class="line">      .reduceByKey((a,b) =&gt; &#123;a + b&#125;)</span><br><span class="line">      .map(x =&gt; &#123;                          <span class="comment">//去除前缀</span></span><br><span class="line">        <span class="keyword">val</span> word = x._1.split(<span class="string">&quot;_&quot;</span>)(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">val</span> count = x._2</span><br><span class="line">        (word, count)</span><br><span class="line">      &#125;)</span><br><span class="line">      .reduceByKey((a,b) =&gt; &#123;a + b&#125;)       <span class="comment">//总聚合</span></span><br><span class="line"></span><br><span class="line">    println(result.collect().mkString(<span class="string">&quot;Array(&quot;</span>, <span class="string">&quot;, &quot;</span>, <span class="string">&quot;)&quot;</span>))</span><br><span class="line"></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Array((scala,1), (hello,275), (spark,3), (hadoop,1))</span><br></pre></td></tr></table></figure></div><footer class="post-footer"><div class="reward-container"><div>请我一杯咖啡吧！</div><button>赞赏</button><div class="post-reward"><div><img src="/../image/wechatpay.png" alt="Canyue 微信"> <span>微信</span></div><div><img src="/../image/alipay.png" alt="Canyue 支付宝"> <span>支付宝</span></div></div></div><div class="post-copyright"><ul><li class="post-copyright-author"><strong>本文作者： </strong>Canyue</li><li class="post-copyright-link"><strong>本文链接：</strong> <a href="https://blog.canyue.top/article/SparkRDD/" title="SparkRDD">https://blog.canyue.top/article/SparkRDD/</a></li><li class="post-copyright-license"><strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/zh_CN" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><div class="post-tags"><a href="/tags/Spark/" rel="tag"># Spark</a> <a href="/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/" rel="tag"># 大数据</a></div><div class="post-nav"><div class="post-nav-item"><a href="/article/Flink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86-StateAPI/" rel="prev" title="Flink状态管理_StateAPI"><i class="fa fa-angle-left"></i> Flink状态管理_StateAPI</a></div><div class="post-nav-item"></div></div></footer></article></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; 2023 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-code"></i> </span><span class="author" itemprop="copyrightHolder">Canyue</span></div>使用Cloudflare、BootCDN为本站加速 <span>博客已"稳定"运行 <span id="days" style="font-weight:800;color:#aaa">0</span>天</span><script>let s1="2023-6-18",days=(s1=new Date(s1.replace(/-/g,"/")),(s2=new Date).getTime()-s1.getTime()),number_of_days=parseInt(days/864e5);document.getElementById("days").innerHTML=number_of_days</script><script color="130,90,120" opacity="0.25" zindex="5" count="60" src="https://cdn.bootcdn.net/ajax/libs/canvas-nest.js/2.0.4/canvas-nest.js"></script></div></footer><div class="back-to-top" role="button" aria-label="返回顶部"><i class="fa fa-arrow-up fa-lg"></i> <span>0%</span></div><div class="reading-progress-bar"></div><noscript><div class="noscript-warning">Theme NexT works best with JavaScript enabled</div></noscript><script src="https://cdn.bootcdn.net/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script><script src="https://cdn.bootcdn.net/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script><script src="https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.33/fancybox/fancybox.umd.js" integrity="sha256-+2+qOqR8CKoHh/AsVR9k2qaDBKWjYNC2nozhYmv5j9k=" crossorigin="anonymous"></script><script src="https://cdn.bootcdn.net/ajax/libs/lozad.js/1.16.0/lozad.min.js" integrity="sha256-mOFREFhqmHeQbXpK2lp4nA3qooVgACfh88fpJftLBbc=" crossorigin="anonymous"></script><script src="https://cdn.bootcdn.net/ajax/libs/pangu/4.0.7/pangu.min.js" integrity="sha256-j+yj56cdEY2CwkVtGyz18fNybFGpMGJ8JxG3GSyO2+I=" crossorigin="anonymous"></script><script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script><script src="https://cdn.bootcdn.net/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script><script src="/js/third-party/search/local-search.js"></script><script src="/js/third-party/fancybox.js"></script><script src="/js/third-party/pace.js"></script><script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script><script src="/js/third-party/math/mathjax.js"></script></body></html>